# FinLlama

Why we can use LLMs to solve certain problem? Refer to [link](#the-first-principles-of-llms).





# the first principles of LLMs

1. Transfer Learning and Pre-training
LLMs are typically pre-trained on vast amounts of text data. This pre-training involves learning a general understanding of language, including grammar, context, and semantics, without being tailored to any specific task. **The idea is that a model that understands language well can then be fine-tuned on a smaller, task-specific dataset to perform a wide range of NLP tasks effectively.**

2. Self-supervised Learning
During pre-training, LLMs often use self-supervised learning techniques. **This means they generate their own labels from the input data.** For example, a common pre-training task is predicting the next word in a sentence given the previous words (as in GPT) or predicting a masked word in a sentence given the surrounding context (as in BERT). This approach allows LLMs to learn from the structure and meaning of language without needing explicit, task-specific annotations.

3. Attention Mechanisms
LLMs heavily rely on attention mechanisms, particularly the Transformer architecture, which allows models to weigh the importance of different words in a sentence or document relative to each other. The self-attention mechanism enables the model to focus on relevant parts of the input when performing a task, improving its ability to understand context and relationships between words.


4. Scalability and Model Size
A core principle behind LLMs is that model performance often improves with scale. This means that larger models, with more parameters, trained on more data, tend to perform better on NLP tasks. This principle has driven the development of increasingly large models, although it's also acknowledged that there are diminishing returns and increased computational costs associated with scaling up.


5. Generalization across Tasks
LLMs are designed to generalize across a wide range of NLP tasks with minimal task-specific adaptation. The goal is for a single model to be capable of performing tasks as diverse as text classification, question answering, text generation, and translation, among others, leveraging its broad understanding of language acquired during pre-training.


6. Ethical and Responsible AI Use
As LLMs have grown in capability and influence, principles around ethical use, fairness, privacy, and transparency have become increasingly important. This includes addressing biases in training data, ensuring models do not generate harmful or misleading content, and being transparent about models' limitations.


# LoRA implementation

used for decrease the parameters

LoRA, which stands for Low-Rank Adaptation, is a technique introduced **to adapt large pre-trained models like those based on the Transformer architecture (e.g., BERT, GPT) with a relatively small increase in the number of trainable parameters.** This approach enables fine-tuning large language models (LLMs) on downstream tasks more efficiently and effectively. The key idea behind LoRA is to update only a small fraction of the model's parameters during the fine-tuning process, thus reducing the computational cost and memory footprint.


# Judge the model is valid:

according to daily performance:

$S_t = \frac{1}{N_t} * \sum_{i=1}^{N_t} S_{it}$

five methods:
![five methods](./pic/image-1.png)
introduce before

For each method, companies were ranked daily according to their sentiment. Companies that did not have sentiment data on a particular day were omitted from the ranking.

The number of hold for short or for long is equal and 35% for long, 35% for short, 30% hold previous position.

$R_{\text{daily}}(i) = r_{\text{long}}(i) - r_{\text{short}}(i)$


Compare the performance of the portfolio constructed using model assess against using other SOTA sentiment methods.

metrics are:
cumulative returns, annualized return, annualized volatility, and the Sharpe ratio


